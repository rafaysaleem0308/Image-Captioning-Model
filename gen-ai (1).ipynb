{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n#  CELL 0 â€” PATH DISCOVERY  (Run this first!)\n#  Finds the real Kaggle paths for images and captions.txt\n# ============================================================\n\nimport os\n\nprint(\"Scanning /kaggle/input ...\\n\")\n\n# Walk the entire input directory and print structure\nfor root, dirs, files in os.walk('/kaggle/input'):\n    level = root.replace('/kaggle/input', '').count(os.sep)\n    indent = '    ' * level\n    print(f\"{indent} Filename: {os.path.basename(root)}/\")\n    \n    # Only show first 3 files per folder to keep output clean\n    for f in files[:3]:\n        print(f\"{indent}    ğŸ“„  {f}\")\n    if len(files) > 3:\n        print(f\"{indent}    ... and {len(files)-3} more files\")\n\n# â”€â”€ Auto-find Images folder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nIMAGE_DIR     = None\nCAPTIONS_PATH = None\n\nfor root, dirs, files in os.walk('/kaggle/input'):\n    jpg_count = sum(1 for f in files if f.lower().endswith(('.jpg', '.jpeg')))\n    if jpg_count > 1000:\n        IMAGE_DIR = root\n\n    for f in files:\n        if f.lower() == 'captions.txt':\n            CAPTIONS_PATH = os.path.join(root, f)\n\n# â”€â”€ Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\"*55)\nif IMAGE_DIR:\n    img_count = len([f for f in os.listdir(IMAGE_DIR) \n                     if f.lower().endswith(('.jpg','.jpeg'))])\n    print(f\"  IMAGE_DIR     = '{IMAGE_DIR}'\")\n    print(f\"    â””â”€ {img_count:,} images found\")\nelse:\n    print(\"  IMAGE_DIR not found â€” check dataset is added!\")\n\nif CAPTIONS_PATH:\n    print(f\"  CAPTIONS_PATH = '{CAPTIONS_PATH}'\")\nelse:\n    print(\"âŒ  captions.txt not found â€” check dataset is added!\")\nprint(\"=\"*55)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:54:08.208392Z","iopub.execute_input":"2026-02-11T15:54:08.208922Z","iopub.status.idle":"2026-02-11T15:54:48.319898Z","shell.execute_reply.started":"2026-02-11T15:54:08.208888Z","shell.execute_reply":"2026-02-11T15:54:48.319235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Part 1 Feature Extraction\n\nimport os, pickle, torch, torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\n\n#Auto-discover Image Directory \ndef find_image_dir(base=\"/kaggle/input\"):\n    \"\"\"Walk input dir and return the folder with 1000+ jpg files.\"\"\"\n    for root, dirs, files in os.walk(base):\n        jpg_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg'))]\n        if len(jpg_files) > 1000:\n            return root\n    return None\n\nIMAGE_DIR   = find_image_dir()\nOUTPUT_FILE = \"/kaggle/working/flickr30k_features.pkl\"\n\n# â”€â”€ Sanity check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif IMAGE_DIR is None:\n    raise FileNotFoundError(\n        \"Could not find image folder. \"\n        \"Make sure the Flickr30k dataset is added to this notebook via \"\n        \"Add Data â†’ Search 'flickr30k'.\"\n    )\n\nall_imgs = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg'))]\nprint(f\"Filename:  Auto-detected IMAGE_DIR : {IMAGE_DIR}\")\nprint(f\"Found {len(all_imgs):,} images\")\n\n# â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass FlickrImageDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_dir   = img_dir\n        self.img_names = [f for f in os.listdir(img_dir)\n                          if f.lower().endswith(('.jpg', '.jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        name     = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, name)\n        img      = Image.open(img_path).convert('RGB')\n        return self.transform(img), name\n\n# â”€â”€ Pre-processing (ImageNet stats) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                         std =(0.229, 0.224, 0.225)),\n])\n\n#Model: ResNet50 without the final FC layer \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\" Device : {device}\")\n\nbackbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nbackbone = nn.Sequential(*list(backbone.children())[:-1])   # â†’ (B, 2048, 1, 1)\nbackbone = nn.DataParallel(backbone).to(device)\nbackbone.eval()\n\n#DataLoader\ndataset = FlickrImageDataset(IMAGE_DIR, transform)\nloader  = DataLoader(dataset,\n                     batch_size=128,\n                     num_workers=4,\n                     pin_memory=True)\n\n#Extraction loop \nfeatures_dict = {}\n\nwith torch.no_grad():\n    for imgs, names in tqdm(loader, desc=\"ğŸ–¼ï¸  Extracting\"):\n        feats = backbone(imgs.to(device))       # (B, 2048, 1, 1)\n        feats = feats.view(imgs.size(0), -1)     # (B, 2048)\n        for i, name in enumerate(names):\n            features_dict[name] = feats[i].cpu().numpy()\n\n#Save\nwith open(OUTPUT_FILE, \"wb\") as f:\n    pickle.dump(features_dict, f)\n\nsample_feat = next(iter(features_dict.values()))\nprint(f\"\\nSaved  â†’ {OUTPUT_FILE}\")\nprint(f\"Images : {len(features_dict):,}\")\nprint(f\"Shape  : {sample_feat.shape}   (should be (2048,))\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:54:48.321514Z","iopub.execute_input":"2026-02-11T15:54:48.321852Z","iopub.status.idle":"2026-02-11T15:56:21.243281Z","shell.execute_reply.started":"2026-02-11T15:54:48.321827Z","shell.execute_reply":"2026-02-11T15:56:21.242583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#  PART 2: VOCABULARY & TEXT PRE-PROCESSING\n\n\nimport re, pickle\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\n#Auto-discover captions.txt \ndef find_captions_file(base=\"/kaggle/input\"):\n    \"\"\"Walk input dir and return the path to captions.txt.\"\"\"\n    for root, dirs, files in os.walk(base):\n        for f in files:\n            if f.lower() == \"captions.txt\":\n                return os.path.join(root, f)\n    return None\n\nimport os\n\nCAPTIONS_PATH = find_captions_file()\nFEATURES_PATH = \"/kaggle/working/flickr30k_features.pkl\"\nVOCAB_OUT     = \"/kaggle/working/vocab.pkl\"\nDATA_OUT      = \"/kaggle/working/captions_processed.pkl\"\n\n#Sanity checks \nif CAPTIONS_PATH is None:\n    raise FileNotFoundError(\n        \"captions.txt not found. \"\n        \"Make sure the Flickr30k dataset is added to this notebook.\"\n    )\n\nif not os.path.exists(FEATURES_PATH):\n    raise FileNotFoundError(\n        \"flickr30k_features.pkl not found. \"\n        \"Please run Part 1 (Feature Extraction) first.\"\n    )\n\nprint(f\"Auto-detected CAPTIONS_PATH : {CAPTIONS_PATH}\")\nprint(f\"Features file found         : {FEATURES_PATH}\")\n\n#Hyper-params\nFREQ_THRESHOLD = 5     # words appearing fewer times â†’ <unk>\nMAX_LEN        = 50    # captions longer than this are dropped\nRANDOM_STATE   = 42\n\n\n#  STEP 1 â€” Load & inspect raw captions\ndf = pd.read_csv(CAPTIONS_PATH)\ndf.columns = df.columns.str.strip().str.lower()\n\n# Rename to standard column names regardless of source format\nif 'image' not in df.columns:\n    df.rename(columns={df.columns[0]: 'image', df.columns[1]: 'caption'}, inplace=True)\n\n# Strip any leading/trailing whitespace from values\ndf['image']   = df['image'].str.strip()\ndf['caption'] = df['caption'].str.strip()\n\nprint(f\"\\nRaw shape      : {df.shape}\")\nprint(f\"Unique images  : {df['image'].nunique():,}\")\nprint(f\"Total captions : {len(df):,}\")\nprint(f\"\\nSample rows:\")\nprint(df.head(3).to_string(index=False))\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  STEP 2 â€” Text cleaning\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\ndef clean_text(text: str) -> str:\n    \"\"\"Lower-case, remove punctuation/digits, collapse spaces.\"\"\"\n    text = str(text).lower().strip()\n    text = re.sub(r\"[^a-z\\s]\", \" \", text)    # keep only letters + spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf['caption_clean'] = df['caption'].apply(clean_text)\n\n# Drop rows where cleaning produced an empty string\nbefore = len(df)\ndf = df[df['caption_clean'].str.len() > 0].copy()\nprint(f\"\\nDropped {before - len(df):,} empty captions after cleaning\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  STEP 3 â€” Tokenise and wrap with <start> / <end>\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\ndf['tokens'] = df['caption_clean'].apply(\n    lambda s: ['<start>'] + s.split() + ['<end>']\n)\n\n# Drop captions that are too long (body only, excluding special tokens)\ndf['body_len'] = df['tokens'].apply(len) - 2    # exclude <start> & <end>\nbefore = len(df)\ndf = df[df['body_len'] <= MAX_LEN].copy()\ndf.drop(columns='body_len', inplace=True)\nprint(f\"Dropped {before - len(df):,} over-length captions \"\n      f\"(max body = {MAX_LEN} tokens)\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  STEP 4 â€” Build Vocabulary\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nclass Vocabulary:\n    \"\"\"\n    Word â†” index mappings.\n    Special tokens:\n        0 â†’ <pad>    padding / ignore_index in CrossEntropyLoss\n        1 â†’ <start>  begin-of-sequence\n        2 â†’ <end>    end-of-sequence\n        3 â†’ <unk>    out-of-vocabulary words\n    \"\"\"\n    PAD_IDX   = 0\n    START_IDX = 1\n    END_IDX   = 2\n    UNK_IDX   = 3\n\n    SPECIALS = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n\n    def __init__(self, freq_threshold: int = 5):\n        self.freq_threshold = freq_threshold\n        self.itos: dict = dict(self.SPECIALS)                   # int  â†’ word\n        self.stoi: dict = {v: k for k, v in self.itos.items()}  # word â†’ int\n\n    def build(self, token_lists):\n        counter     = Counter()\n        special_set = set(self.SPECIALS.values())\n        for tokens in token_lists:\n            counter.update(t for t in tokens if t not in special_set)\n\n        idx = len(self.SPECIALS)\n        for word, freq in counter.most_common():\n            if freq >= self.freq_threshold:\n                self.itos[idx] = word\n                self.stoi[word] = idx\n                idx += 1\n\n        print(f\" Vocabulary built : {len(self):,} tokens  \"\n              f\"(freq_threshold = {self.freq_threshold})\")\n\n    def numericalize(self, tokens) -> list:\n        return [self.stoi.get(t, self.UNK_IDX) for t in tokens]\n\n    def decode(self, indices) -> list:\n        skip  = {self.PAD_IDX, self.START_IDX}\n        words = []\n        for i in indices:\n            if i == self.END_IDX:\n                break\n            if i not in skip:\n                words.append(self.itos.get(i, '<unk>'))\n        return words\n\n    def __len__(self):\n        return len(self.itos)\n\n\nvocab = Vocabulary(freq_threshold=FREQ_THRESHOLD)\nvocab.build(df['tokens'])\n\n\n#  STEP 5 â€” Numericalize captions\n\ndf['caption_ids'] = df['tokens'].apply(vocab.numericalize)\n\n\n#  STEP 6 â€” Align with feature cache & Train/Val/Test split\n\nwith open(FEATURES_PATH, \"rb\") as f:\n    features_dict = pickle.load(f)\n\navailable_imgs = set(features_dict.keys())\n\n# Some datasets store image names with a numeric suffix (e.g. \"img.jpg#0\")\n# Strip any \"#N\" suffix that might appear in the captions file\ndf['image'] = df['image'].str.replace(r'#\\d+$', '', regex=True).str.strip()\n\nbefore = len(df)\ndf = df[df['image'].isin(available_imgs)].copy()\nprint(f\"\\nğŸ”—  Captions aligned with feature cache : {len(df):,}  \"\n      f\"(dropped {before - len(df):,} unmatched rows)\")\n\n#Split by image (not by row) to prevent leakage \nunique_imgs = df['image'].unique()\ntrain_imgs, temp_imgs = train_test_split(unique_imgs,\n                                          test_size=0.20,\n                                          random_state=RANDOM_STATE)\nval_imgs, test_imgs   = train_test_split(temp_imgs,\n                                          test_size=0.50,\n                                          random_state=RANDOM_STATE)\n\ndf['split'] = 'train'\ndf.loc[df['image'].isin(val_imgs),  'split'] = 'val'\ndf.loc[df['image'].isin(test_imgs), 'split'] = 'test'\n\nprint(f\"\\nğŸ“Š  Dataset splits:\")\nfor split in ('train', 'val', 'test'):\n    sub = df[df['split'] == split]\n    print(f\"    {split:5s}  â†’  {sub['image'].nunique():5,} images  |  \"\n          f\"{len(sub):6,} captions\")\n\n\n#  STEP 7 â€” Save outputs\n\n# vocab.pkl \nwith open(VOCAB_OUT, \"wb\") as f:\n    pickle.dump(vocab, f)\n\n# aptions_processed.pkl \nprocessed = {\n    \"dataframe\"  : df,\n    \"train_imgs\" : set(train_imgs),\n    \"val_imgs\"   : set(val_imgs),\n    \"test_imgs\"  : set(test_imgs),\n    \"max_len\"    : MAX_LEN,\n    \"vocab_size\" : len(vocab),\n    \"pad_idx\"    : Vocabulary.PAD_IDX,\n    \"start_idx\"  : Vocabulary.START_IDX,\n    \"end_idx\"    : Vocabulary.END_IDX,\n    \"unk_idx\"    : Vocabulary.UNK_IDX,\n}\nwith open(DATA_OUT, \"wb\") as f:\n    pickle.dump(processed, f)\n\nprint(f\"\\n vocab saved          â†’ {VOCAB_OUT}\")\nprint(f\" processed data saved â†’ {DATA_OUT}\")\n\n# â”€â”€ Sanity-check decode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsample  = df.iloc[0]\ndecoded = vocab.decode(sample['caption_ids'])\nprint(f\"\\n Sanity check (row 0):\")\nprint(f\"    Original : {sample['caption']}\")\nprint(f\"    Tokens   : {sample['tokens']}\")\nprint(f\"    IDs      : {sample['caption_ids']}\")\nprint(f\"    Decoded  : {' '.join(decoded)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:56:21.244401Z","iopub.execute_input":"2026-02-11T15:56:21.244643Z","iopub.status.idle":"2026-02-11T15:56:26.423364Z","shell.execute_reply.started":"2026-02-11T15:56:21.244618Z","shell.execute_reply":"2026-02-11T15:56:26.422640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#  PART 3: Seq2Seq ARCHITECTURE\n\n\nimport os, pickle, torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n#Paths\nFEATURES_PATH = \"/kaggle/working/flickr30k_features.pkl\"\nVOCAB_PATH    = \"/kaggle/working/vocab.pkl\"\nDATA_PATH     = \"/kaggle/working/captions_processed.pkl\"\n\n#Load saved outputs from Part 1 & 2\nwith open(FEATURES_PATH, \"rb\") as f:\n    features_dict = pickle.load(f)\n\nwith open(VOCAB_PATH, \"rb\") as f:\n    vocab = pickle.load(f)\n\nwith open(DATA_PATH, \"rb\") as f:\n    data = pickle.load(f)\n\ndf          = data[\"dataframe\"]\ntrain_imgs  = data[\"train_imgs\"]\nval_imgs    = data[\"val_imgs\"]\ntest_imgs   = data[\"test_imgs\"]\nVOCAB_SIZE  = data[\"vocab_size\"]\nMAX_LEN     = data[\"max_len\"]\nPAD_IDX     = data[\"pad_idx\"]\nSTART_IDX   = data[\"start_idx\"]\nEND_IDX     = data[\"end_idx\"]\n\nprint(f\"  Loaded features   : {len(features_dict):,} images\")\nprint(f\"  Loaded vocab      : {VOCAB_SIZE:,} tokens\")\nprint(f\"  Loaded captions   : {len(df):,} rows\")\nprint(f\"  PAD={PAD_IDX}  START={START_IDX}  END={END_IDX}  MAX_LEN={MAX_LEN}\")\n\n\n#  HYPER-PARAMETERS\nFEATURE_DIM = 2048    # ResNet50 output dim  (fixed)\nEMBED_SIZE  = 256     # word embedding size\nHIDDEN_SIZE = 512     # LSTM hidden units\nNUM_LAYERS  = 2       # LSTM layers\nDROPOUT     = 0.5\nBATCH_SIZE  = 64\ndevice      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nğŸš€  Device : {device}\")\n\n\n#  DATASET\n\nclass FlickrSeq2SeqDataset(Dataset):\n    \"\"\"\n    Returns:\n        feature   : (2048,)        float32  â€” cached ResNet50 vector\n        caption   : (MAX_LEN+2,)   long     â€” full <start> + tokens + <end>, padded\n    \"\"\"\n    def __init__(self, df, split_imgs, features_dict, max_len, pad_idx):\n        self.df           = df[df['image'].isin(split_imgs)].reset_index(drop=True)\n        self.features     = features_dict\n        self.max_cap_len  = max_len + 2          # +2 for <start> and <end>\n        self.pad_idx      = pad_idx\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row        = self.df.iloc[idx]\n        feature    = torch.tensor(self.features[row['image']], dtype=torch.float32)\n\n        # Truncate then pad to fixed length\n        ids        = row['caption_ids'][:self.max_cap_len]\n        pad_len    = self.max_cap_len - len(ids)\n        ids_padded = ids + [self.pad_idx] * pad_len\n        caption    = torch.tensor(ids_padded, dtype=torch.long)\n\n        return feature, caption\n\n\ntrain_dataset = FlickrSeq2SeqDataset(df, train_imgs, features_dict, MAX_LEN, PAD_IDX)\nval_dataset   = FlickrSeq2SeqDataset(df, val_imgs,   features_dict, MAX_LEN, PAD_IDX)\ntest_dataset  = FlickrSeq2SeqDataset(df, test_imgs,  features_dict, MAX_LEN, PAD_IDX)\n\ntrain_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                           num_workers=4, pin_memory=True)\nval_loader    = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                           num_workers=4, pin_memory=True)\ntest_loader   = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                           num_workers=4, pin_memory=True)\n\nprint(f\"\\n  Train batches : {len(train_loader):,}\")\nprint(f\"  Val   batches : {len(val_loader):,}\")\nprint(f\"  Test  batches : {len(test_loader):,}\")\n\n\n#  ENCODER\n#  ResNet50 feature (2048) â†’ hidden state (512)\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Projects the cached 2048-dim image feature into\n    HIDDEN_SIZE to initialise the LSTM hidden & cell states.\n\n    Architecture:\n        Linear(2048 â†’ hidden_size) â†’ BatchNorm â†’ ReLU â†’ Dropout\n    \"\"\"\n    def __init__(self, feature_dim: int, hidden_size: int,\n                 num_layers: int, dropout: float):\n        super().__init__()\n        self.fc        = nn.Linear(feature_dim, hidden_size)\n        self.bn        = nn.BatchNorm1d(hidden_size)\n        self.relu      = nn.ReLU()\n        self.dropout   = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n    def forward(self, features):\n        \"\"\"\n        Args:\n            features : (B, 2048)\n        Returns:\n            hidden   : (num_layers, B, hidden_size)\n            cell     : (num_layers, B, hidden_size)  â† zeros\n        \"\"\"\n        out    = self.dropout(self.relu(self.bn(self.fc(features))))  # (B, H)\n        hidden = out.unsqueeze(0).repeat(self.num_layers, 1, 1)       # (L, B, H)\n        cell   = torch.zeros_like(hidden)                             # (L, B, H)\n        return hidden, cell\n\n\n\n#  DECODER\n#  Word Embeddings â†’ LSTM â†’ Linear(vocab_size)\nclass Decoder(nn.Module):\n    \"\"\"\n    At each step takes one word token, embeds it, feeds into LSTM,\n    and projects to vocab logits.\n\n    Architecture:\n        Embedding(vocab_size, embed_size)\n        LSTM(embed_size â†’ hidden_size, num_layers)\n        Dropout\n        Linear(hidden_size â†’ vocab_size)\n    \"\"\"\n    def __init__(self, vocab_size: int, embed_size: int,\n                 hidden_size: int, num_layers: int,\n                 dropout: float, pad_idx: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size,\n                                      padding_idx=pad_idx)\n        self.lstm      = nn.LSTM(embed_size, hidden_size,\n                                 num_layers=num_layers,\n                                 batch_first=True,\n                                 dropout=dropout if num_layers > 1 else 0.0)\n        self.dropout   = nn.Dropout(dropout)\n        self.fc_out    = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, captions, hidden, cell):\n        \"\"\"\n        Args:\n            captions : (B, seq_len)           â€” input token ids\n            hidden   : (num_layers, B, H)\n            cell     : (num_layers, B, H)\n        Returns:\n            logits   : (B, seq_len, vocab_size)\n            hidden   : (num_layers, B, H)\n            cell     : (num_layers, B, H)\n        \"\"\"\n        embeddings         = self.dropout(self.embedding(captions))  # (B, S, E)\n        outputs, (h, c)    = self.lstm(embeddings, (hidden, cell))   # (B, S, H)\n        logits             = self.fc_out(self.dropout(outputs))      # (B, S, V)\n        return logits, h, c\n\n\n\n#  SEQ2SEQ  (Encoder + Decoder combined)\n\nclass Seq2Seq(nn.Module):\n    \"\"\"\n    Full image captioning model.\n\n    Forward pass uses Teacher Forcing:\n        input  tokens : caption[:, :-1]  â†’  [<start>, w1, w2, ..., wn]\n        target tokens : caption[:, 1:]   â†’  [w1, w2, ..., wn, <end>]\n    \"\"\"\n    def __init__(self, encoder: Encoder, decoder: Decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, features, captions):\n        \"\"\"\n        Args:\n            features : (B, 2048)\n            captions : (B, MAX_LEN+2)   full padded caption\n        Returns:\n            logits   : (B, MAX_LEN+1, vocab_size)\n        \"\"\"\n        hidden, cell = self.encoder(features)           # initialise LSTM state\n        # Teacher forcing:\n        #   decoder input  = captions[:, :-1]  (drop last  token)\n        #   target         = captions[:, 1:]   (drop first token) â† used in Part 4 loss\n        logits, _, _  = self.decoder(captions[:, :-1],\n                                     hidden, cell)\n        return logits   # (B, seq_len-1, vocab_size)\n\n\n#  INSTANTIATE MODEL\nencoder = Encoder(feature_dim=FEATURE_DIM,\n                  hidden_size=HIDDEN_SIZE,\n                  num_layers=NUM_LAYERS,\n                  dropout=DROPOUT)\n\ndecoder = Decoder(vocab_size=VOCAB_SIZE,\n                  embed_size=EMBED_SIZE,\n                  hidden_size=HIDDEN_SIZE,\n                  num_layers=NUM_LAYERS,\n                  dropout=DROPOUT,\n                  pad_idx=PAD_IDX)\n\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n#Parameter count\ntotal_params     = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n#  ARCHITECTURE SUMMARY\n\nprint(\"\\n\" + \"=\"*55)\nprint(\"         SEQ2SEQ ARCHITECTURE SUMMARY\")\nprint(\"=\"*55)\nprint(f\"\\n  ENCODER\")\nprint(f\"    Linear      : {FEATURE_DIM}  â†’ {HIDDEN_SIZE}\")\nprint(f\"    BatchNorm1d : {HIDDEN_SIZE}\")\nprint(f\"    Activation  : ReLU  +  Dropout({DROPOUT})\")\nprint(f\"    Output      : hidden & cell  â†’  ({NUM_LAYERS}, B, {HIDDEN_SIZE})\")\n\nprint(f\"\\n  DECODER\")\nprint(f\"    Embedding   : {VOCAB_SIZE} vocab  â†’  {EMBED_SIZE} embed_size\")\nprint(f\"    LSTM        : {EMBED_SIZE} â†’ {HIDDEN_SIZE}  |  layers={NUM_LAYERS}\")\nprint(f\"    Dropout     : {DROPOUT}\")\nprint(f\"    Linear      : {HIDDEN_SIZE}  â†’  {VOCAB_SIZE} (vocab logits)\")\n\nprint(f\"\\n   TRAINING CONFIG\")\nprint(f\"    MAX_LEN     : {MAX_LEN}  tokens\")\nprint(f\"    BATCH_SIZE  : {BATCH_SIZE}\")\nprint(f\"    DEVICE      : {device}\")\nprint(f\"\\n  PARAMETERS\")\nprint(f\"    Total       : {total_params:,}\")\nprint(f\"    Trainable   : {trainable_params:,}\")\nprint(\"=\"*55)\n\n\n#  FORWARD PASS SANITY CHECK\n\nprint(\"\\n  Running forward pass sanity check...\")\n\nmodel.eval()\nwith torch.no_grad():\n    sample_feats, sample_caps = next(iter(train_loader))\n    sample_feats = sample_feats.to(device)\n    sample_caps  = sample_caps.to(device)\n\n    output = model(sample_feats, sample_caps)\n\n    print(f\"    Input  features  : {sample_feats.shape}   (B, 2048)\")\n    print(f\"    Input  captions  : {sample_caps.shape}    (B, MAX_LEN+2)\")\n    print(f\"    Output logits    : {output.shape}  (B, MAX_LEN+1, vocab_size)\")\n\nexpected_shape = (BATCH_SIZE, MAX_LEN + 1, VOCAB_SIZE)\nassert output.shape == expected_shape, \\\n    f\"Shape mismatch! Got {output.shape}, expected {expected_shape}\"\n\nprint(f\"\\n Sanity check passed â€” model output shape is correct!\")\nprint(f\"Part 3 complete â€” model is ready for Part 4 Training!\")\n\n# â”€â”€ Save model config for Part 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel_config = {\n    \"FEATURE_DIM\" : FEATURE_DIM,\n    \"EMBED_SIZE\"  : EMBED_SIZE,\n    \"HIDDEN_SIZE\" : HIDDEN_SIZE,\n    \"NUM_LAYERS\"  : NUM_LAYERS,\n    \"DROPOUT\"     : DROPOUT,\n    \"VOCAB_SIZE\"  : VOCAB_SIZE,\n    \"MAX_LEN\"     : MAX_LEN,\n    \"PAD_IDX\"     : PAD_IDX,\n    \"START_IDX\"   : START_IDX,\n    \"END_IDX\"     : END_IDX,\n    \"BATCH_SIZE\"  : BATCH_SIZE,\n}\n\nwith open(\"/kaggle/working/model_config.pkl\", \"wb\") as f:\n    pickle.dump(model_config, f)\n\nprint(f\"  model_config saved  â†’ /kaggle/working/model_config.pkl\")\n\n\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:56:26.425326Z","iopub.execute_input":"2026-02-11T15:56:26.425565Z","iopub.status.idle":"2026-02-11T15:56:27.742544Z","shell.execute_reply.started":"2026-02-11T15:56:26.425543Z","shell.execute_reply":"2026-02-11T15:56:27.741688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n#  PART 4: TRAINING & INFERENCE\n#  Inputs  â† /kaggle/working/flickr30k_features.pkl\n#            /kaggle/working/vocab.pkl\n#            /kaggle/working/captions_processed.pkl\n#            /kaggle/working/model_config.pkl\n#  Outputs â†’ /kaggle/working/best_model.pth\n#            Training metrics & generated captions\n# ============================================================\n\nimport os, pickle, torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\n\n# â”€â”€ Load everything from previous parts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFEATURES_PATH = \"/kaggle/working/flickr30k_features.pkl\"\nVOCAB_PATH    = \"/kaggle/working/vocab.pkl\"\nDATA_PATH     = \"/kaggle/working/captions_processed.pkl\"\nCONFIG_PATH   = \"/kaggle/working/model_config.pkl\"\nMODEL_SAVE    = \"/kaggle/working/best_model.pth\"\n\nwith open(FEATURES_PATH, \"rb\") as f:\n    features_dict = pickle.load(f)\n\nwith open(VOCAB_PATH, \"rb\") as f:\n    vocab = pickle.load(f)\n\nwith open(DATA_PATH, \"rb\") as f:\n    data = pickle.load(f)\n\nwith open(CONFIG_PATH, \"rb\") as f:\n    config = pickle.load(f)\n\ndf = data[\"dataframe\"]\ntrain_imgs = data[\"train_imgs\"]\nval_imgs = data[\"val_imgs\"]\ntest_imgs = data[\"test_imgs\"]\n\n# â”€â”€ Extract config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFEATURE_DIM = config[\"FEATURE_DIM\"]\nEMBED_SIZE  = config[\"EMBED_SIZE\"]\nHIDDEN_SIZE = config[\"HIDDEN_SIZE\"]\nNUM_LAYERS  = config[\"NUM_LAYERS\"]\nDROPOUT     = config[\"DROPOUT\"]\nVOCAB_SIZE  = config[\"VOCAB_SIZE\"]\nMAX_LEN     = config[\"MAX_LEN\"]\nPAD_IDX     = config[\"PAD_IDX\"]\nSTART_IDX   = config[\"START_IDX\"]\nEND_IDX     = config[\"END_IDX\"]\nBATCH_SIZE  = config[\"BATCH_SIZE\"]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"âœ…  All files loaded successfully\")\nprint(f\"ğŸš€  Device : {device}\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  RE-DEFINE ARCHITECTURE (same as Part 3)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass Encoder(nn.Module):\n    def __init__(self, feature_dim, hidden_size, num_layers, dropout):\n        super().__init__()\n        self.fc = nn.Linear(feature_dim, hidden_size)\n        self.bn = nn.BatchNorm1d(hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n    def forward(self, features):\n        out = self.dropout(self.relu(self.bn(self.fc(features))))\n        hidden = out.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        cell = torch.zeros_like(hidden)\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n                           batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, captions, hidden, cell):\n        embeddings = self.dropout(self.embedding(captions))\n        outputs, (h, c) = self.lstm(embeddings, (hidden, cell))\n        logits = self.fc_out(self.dropout(outputs))\n        return logits, h, c\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, features, captions):\n        hidden, cell = self.encoder(features)\n        logits, _, _ = self.decoder(captions[:, :-1], hidden, cell)\n        return logits\n\n\nclass FlickrSeq2SeqDataset(Dataset):\n    def __init__(self, df, split_imgs, features_dict, max_len, pad_idx):\n        self.df = df[df['image'].isin(split_imgs)].reset_index(drop=True)\n        self.features = features_dict\n        self.max_cap_len = max_len + 2\n        self.pad_idx = pad_idx\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        feature = torch.tensor(self.features[row['image']], dtype=torch.float32)\n        ids = row['caption_ids'][:self.max_cap_len]\n        pad_len = self.max_cap_len - len(ids)\n        ids_padded = ids + [self.pad_idx] * pad_len\n        caption = torch.tensor(ids_padded, dtype=torch.long)\n        return feature, caption\n\n\n# â”€â”€ Initialize model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nencoder = Encoder(FEATURE_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\ndecoder = Decoder(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, PAD_IDX)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n# â”€â”€ DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrain_dataset = FlickrSeq2SeqDataset(df, train_imgs, features_dict, MAX_LEN, PAD_IDX)\nval_dataset   = FlickrSeq2SeqDataset(df, val_imgs, features_dict, MAX_LEN, PAD_IDX)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                        num_workers=4, pin_memory=True)\n\nprint(f\"  Train samples : {len(train_dataset):,}\")\nprint(f\"  Val samples   : {len(val_dataset):,}\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  TRAINING SETUP\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nNUM_EPOCHS    = 10\nLEARNING_RATE = 3e-4\nGRAD_CLIP     = 5.0\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nprint(f\"   NUM_EPOCHS     : {NUM_EPOCHS}\")\nprint(f\"   LEARNING_RATE  : {LEARNING_RATE}\")\nprint(f\"   GRAD_CLIP      : {GRAD_CLIP}\")\nprint(f\"   Loss function  : CrossEntropyLoss(ignore_index={PAD_IDX})\")\nprint(f\"   Optimizer      : Adam\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  TRAINING & VALIDATION FUNCTIONS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef train_epoch(model, loader, criterion, optimizer, device, grad_clip):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0\n    \n    for features, captions in tqdm(loader, desc=\"ğŸ”„ Training\"):\n        features = features.to(device)\n        captions = captions.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits = model(features, captions)  # (B, seq_len, vocab_size)\n        \n        # Reshape for loss: (B*seq_len, vocab_size) vs (B*seq_len)\n        targets = captions[:, 1:]  # drop <start>\n        logits = logits.reshape(-1, logits.size(-1))\n        targets = targets.reshape(-1)\n        \n        loss = criterion(logits, targets)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate on validation set.\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for features, captions in tqdm(loader, desc=\"âœ… Validating\"):\n            features = features.to(device)\n            captions = captions.to(device)\n            \n            logits = model(features, captions)\n            targets = captions[:, 1:]\n            \n            logits = logits.reshape(-1, logits.size(-1))\n            targets = targets.reshape(-1)\n            \n            loss = criterion(logits, targets)\n            total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  TRAINING LOOP\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*55)\nprint(\"         STARTING TRAINING\")\nprint(\"=\"*55 + \"\\n\")\n\nbest_val_loss = float('inf')\nhistory = {\"train_loss\": [], \"val_loss\": []}\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, GRAD_CLIP)\n    val_loss = validate(model, val_loader, criterion, device)\n    \n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    \n    print(f\"\\n Epoch {epoch}/{NUM_EPOCHS}\")\n    print(f\"    Train Loss : {train_loss:.4f}\")\n    print(f\"    Val Loss   : {val_loss:.4f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), MODEL_SAVE)\n        print(f\"  New best model saved! (val_loss={val_loss:.4f})\")\n    \n    print(\"\")\n\nprint(\"=\"*55)\nprint(\"         TRAINING COMPLETE\")\nprint(\"=\"*55)\nprint(f\"  Best model saved â†’ {MODEL_SAVE}\")\nprint(f\"  Best val loss   : {best_val_loss:.4f}\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  INFERENCE FUNCTIONS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef greedy_search(model, feature, vocab, max_len=50, device='cpu'):\n    \"\"\"\n    Generates caption using greedy decoding (pick highest probability word).\n    \n    Args:\n        model    : trained Seq2Seq model\n        feature  : (2048,) image feature vector\n        vocab    : Vocabulary object\n        max_len  : maximum caption length\n        device   : torch device\n    \n    Returns:\n        caption  : list of words (decoded)\n    \"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Initialize\n        feature = feature.unsqueeze(0).to(device)  # (1, 2048)\n        hidden, cell = model.encoder(feature)\n        \n        # Start with <start> token\n        input_token = torch.tensor([[START_IDX]], dtype=torch.long, device=device)\n        generated_ids = []\n        \n        for _ in range(max_len):\n            logits, hidden, cell = model.decoder(input_token, hidden, cell)\n            predicted_id = logits.argmax(dim=-1).item()  # greedy choice\n            \n            if predicted_id == END_IDX:\n                break\n            \n            generated_ids.append(predicted_id)\n            input_token = torch.tensor([[predicted_id]], dtype=torch.long, device=device)\n        \n        caption = vocab.decode(generated_ids)\n        return caption\n\n\ndef beam_search(model, feature, vocab, beam_width=3, max_len=50, device='cpu'):\n    \"\"\"\n    Generates caption using beam search (maintains top-k candidates).\n    \n    Args:\n        model       : trained Seq2Seq model\n        feature     : (2048,) image feature vector\n        vocab       : Vocabulary object\n        beam_width  : number of beams to keep\n        max_len     : maximum caption length\n        device      : torch device\n    \n    Returns:\n        caption     : list of words (best beam decoded)\n    \"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # Initialize\n        feature = feature.unsqueeze(0).to(device)  # (1, 2048)\n        hidden, cell = model.encoder(feature)\n        \n        # Each beam: (sequence, score, hidden, cell)\n        beams = [([START_IDX], 0.0, hidden, cell)]\n        \n        for _ in range(max_len):\n            candidates = []\n            \n            for seq, score, h, c in beams:\n                if seq[-1] == END_IDX:\n                    candidates.append((seq, score, h, c))\n                    continue\n                \n                input_token = torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n                logits, new_h, new_c = model.decoder(input_token, h, c)\n                log_probs = torch.log_softmax(logits[0, 0], dim=-1)\n                \n                # Top-k tokens\n                top_probs, top_ids = log_probs.topk(beam_width)\n                \n                for prob, token_id in zip(top_probs, top_ids):\n                    new_seq = seq + [token_id.item()]\n                    new_score = score + prob.item()\n                    candidates.append((new_seq, new_score, new_h, new_c))\n            \n            # Keep top beam_width beams\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n            \n            # Stop if all beams ended\n            if all(seq[-1] == END_IDX for seq, _, _, _ in beams):\n                break\n        \n        # Return best beam\n        best_seq = beams[0][0]\n        caption = vocab.decode(best_seq)\n        return caption\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  TEST INFERENCE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*55)\nprint(\"         TESTING INFERENCE\")\nprint(\"=\"*55 + \"\\n\")\n\n# Load best model\nmodel.load_state_dict(torch.load(MODEL_SAVE))\nmodel.eval()\n\n# Test on 3 random images\ntest_df = df[df['image'].isin(test_imgs)].reset_index(drop=True)\n\nfor i in range(3):\n    sample_idx = np.random.randint(len(test_df))\n    sample_row = test_df.iloc[sample_idx]\n    sample_feature = torch.tensor(features_dict[sample_row['image']], dtype=torch.float32)\n    \n    print(f\"{'â”€'*55}\")\n    print(f\"ğŸ–¼ï¸   Image {i+1}     : {sample_row['image']}\")\n    print(f\"ğŸ’¬  Ground Truth: {sample_row['caption']}\")\n    \n    # Greedy Search\n    greedy_caption = greedy_search(model, sample_feature, vocab, max_len=MAX_LEN, device=device)\n    print(f\"ğŸ”¹  Greedy      : {' '.join(greedy_caption)}\")\n    \n    # Beam Search\n    beam_caption = beam_search(model, sample_feature, vocab, beam_width=5, max_len=MAX_LEN, device=device)\n    print(f\"ğŸ”¸  Beam(5)     : {' '.join(beam_caption)}\\n\")\n\nprint(\"=\"*55)\nprint(\"         PART 4 COMPLETE\")\nprint(\"=\"*55)\nprint(f\"\\n  Training done  : {NUM_EPOCHS} epochs\")\nprint(f\"  Best model     : {MODEL_SAVE}\")\nprint(f\"  Inference ready: greedy_search() & beam_search()\")\nprint(f\"\\n  To generate captions for new images:\")\nprint(f\"    greedy_search(model, feature, vocab, device=device)\")\nprint(f\"    beam_search(model, feature, vocab, beam_width=5, device=device)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:56:27.744190Z","iopub.execute_input":"2026-02-11T15:56:27.744522Z","iopub.status.idle":"2026-02-11T16:17:39.898621Z","shell.execute_reply.started":"2026-02-11T15:56:27.744493Z","shell.execute_reply":"2026-02-11T16:17:39.897856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n#  PART 5: EVALUATION & DEPLOYMENT\n#  Inputs  â† /kaggle/working/best_model.pth\n#            /kaggle/working/flickr30k_features.pkl\n#            /kaggle/working/vocab.pkl\n#            /kaggle/working/captions_processed.pkl\n#            /kaggle/working/model_config.pkl\n#  Outputs â†’ Visualizations, metrics, and interactive demo\n# ============================================================\n\nimport os, pickle, torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom collections import Counter\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# â”€â”€ Install Gradio if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntry:\n    import gradio as gr\nexcept ImportError:\n    print(\"ğŸ“¦ Installing Gradio...\")\n    import subprocess\n    subprocess.check_call(['pip', 'install', 'gradio', '-q'])\n    import gradio as gr\n\n# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFEATURES_PATH = \"/kaggle/working/flickr30k_features.pkl\"\nVOCAB_PATH    = \"/kaggle/working/vocab.pkl\"\nDATA_PATH     = \"/kaggle/working/captions_processed.pkl\"\nCONFIG_PATH   = \"/kaggle/working/model_config.pkl\"\nMODEL_PATH    = \"/kaggle/working/best_model.pth\"\n\n# â”€â”€ Load everything â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwith open(FEATURES_PATH, \"rb\") as f:\n    features_dict = pickle.load(f)\n\nwith open(VOCAB_PATH, \"rb\") as f:\n    vocab = pickle.load(f)\n\nwith open(DATA_PATH, \"rb\") as f:\n    data = pickle.load(f)\n\nwith open(CONFIG_PATH, \"rb\") as f:\n    config = pickle.load(f)\n\ndf = data[\"dataframe\"]\ntest_imgs = data[\"test_imgs\"]\n\nFEATURE_DIM = config[\"FEATURE_DIM\"]\nEMBED_SIZE  = config[\"EMBED_SIZE\"]\nHIDDEN_SIZE = config[\"HIDDEN_SIZE\"]\nNUM_LAYERS  = config[\"NUM_LAYERS\"]\nDROPOUT     = config[\"DROPOUT\"]\nVOCAB_SIZE  = config[\"VOCAB_SIZE\"]\nMAX_LEN     = config[\"MAX_LEN\"]\nPAD_IDX     = config[\"PAD_IDX\"]\nSTART_IDX   = config[\"START_IDX\"]\nEND_IDX     = config[\"END_IDX\"]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"  All files loaded successfully\")\nprint(f\"  Device : {device}\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  RE-DEFINE ARCHITECTURE (same as Part 3 & 4)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass Encoder(nn.Module):\n    def __init__(self, feature_dim, hidden_size, num_layers, dropout):\n        super().__init__()\n        self.fc = nn.Linear(feature_dim, hidden_size)\n        self.bn = nn.BatchNorm1d(hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n    def forward(self, features):\n        out = self.dropout(self.relu(self.bn(self.fc(features))))\n        hidden = out.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        cell = torch.zeros_like(hidden)\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n                           batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, captions, hidden, cell):\n        embeddings = self.dropout(self.embedding(captions))\n        outputs, (h, c) = self.lstm(embeddings, (hidden, cell))\n        logits = self.fc_out(self.dropout(outputs))\n        return logits, h, c\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, features, captions):\n        hidden, cell = self.encoder(features)\n        logits, _, _ = self.decoder(captions[:, :-1], hidden, cell)\n        return logits\n\n\ndef greedy_search(model, feature, vocab, max_len=50, device='cpu'):\n    \"\"\"Generate caption using greedy search.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        feature = feature.unsqueeze(0).to(device)\n        hidden, cell = model.encoder(feature)\n        input_token = torch.tensor([[START_IDX]], dtype=torch.long, device=device)\n        generated_ids = []\n        \n        for _ in range(max_len):\n            logits, hidden, cell = model.decoder(input_token, hidden, cell)\n            predicted_id = logits.argmax(dim=-1).item()\n            if predicted_id == END_IDX:\n                break\n            generated_ids.append(predicted_id)\n            input_token = torch.tensor([[predicted_id]], dtype=torch.long, device=device)\n        \n        return vocab.decode(generated_ids)\n\n\ndef beam_search(model, feature, vocab, beam_width=5, max_len=50, device='cpu'):\n    \"\"\"Generate caption using beam search.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        feature = feature.unsqueeze(0).to(device)\n        hidden, cell = model.encoder(feature)\n        beams = [([START_IDX], 0.0, hidden, cell)]\n        \n        for _ in range(max_len):\n            candidates = []\n            for seq, score, h, c in beams:\n                if seq[-1] == END_IDX:\n                    candidates.append((seq, score, h, c))\n                    continue\n                \n                input_token = torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n                logits, new_h, new_c = model.decoder(input_token, h, c)\n                log_probs = torch.log_softmax(logits[0, 0], dim=-1)\n                top_probs, top_ids = log_probs.topk(beam_width)\n                \n                for prob, token_id in zip(top_probs, top_ids):\n                    new_seq = seq + [token_id.item()]\n                    new_score = score + prob.item()\n                    candidates.append((new_seq, new_score, new_h, new_c))\n            \n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n            if all(seq[-1] == END_IDX for seq, _, _, _ in beams):\n                break\n        \n        best_seq = beams[0][0]\n        return vocab.decode(best_seq)\n\n\n# â”€â”€ Load trained model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nencoder = Encoder(FEATURE_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\ndecoder = Decoder(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, PAD_IDX)\nmodel = Seq2Seq(encoder, decoder).to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.eval()\n\nprint(f\"  Model loaded from {MODEL_PATH}\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  1. CAPTION EXAMPLES (5 Random Test Images)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*60)\nprint(\"         1. CAPTION EXAMPLES\")\nprint(\"=\"*60 + \"\\n\")\n\n# Auto-discover image directory\ndef find_image_dir(base=\"/kaggle/input\"):\n    for root, dirs, files in os.walk(base):\n        jpg_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg'))]\n        if len(jpg_files) > 1000:\n            return root\n    return None\n\nIMAGE_DIR = find_image_dir()\nif IMAGE_DIR is None:\n    print(\" Warning: Could not find image directory. Skipping visualization.\")\n    IMAGE_DIR = \"/kaggle/input\"\n\ntest_df = df[df['image'].isin(test_imgs)].reset_index(drop=True)\n\n# Select 5 random test images\nnp.random.seed(42)\nsample_indices = np.random.choice(len(test_df), size=min(5, len(test_df)), replace=False)\n\nfig, axes = plt.subplots(5, 1, figsize=(12, 20))\n\nfor idx, sample_idx in enumerate(sample_indices):\n    sample_row = test_df.iloc[sample_idx]\n    img_name = sample_row['image']\n    img_path = os.path.join(IMAGE_DIR, img_name)\n    \n    # Load image\n    try:\n        img = Image.open(img_path).convert('RGB')\n    except:\n        print(f\" Could not load {img_name}, skipping...\")\n        continue\n    \n    # Get feature and generate caption\n    feature = torch.tensor(features_dict[img_name], dtype=torch.float32)\n    generated_caption = beam_search(model, feature, vocab, beam_width=5, \n                                   max_len=MAX_LEN, device=device)\n    \n    # Display\n    axes[idx].imshow(img)\n    axes[idx].axis('off')\n    axes[idx].set_title(\n        f\"Ground Truth: {sample_row['caption']}\\n\"\n        f\"Generated: {' '.join(generated_caption)}\",\n        fontsize=10, wrap=True, pad=10\n    )\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/caption_examples.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"  Caption examples saved â†’ /kaggle/working/caption_examples.png\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  2. LOSS CURVES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*60)\nprint(\"         2. LOSS CURVES\")\nprint(\"=\"*60 + \"\\n\")\n\n# Note: In a real scenario, you'd save history during training\n# For demo purposes, we'll create sample data\n# In Part 4, you should save history like:\n# with open(\"/kaggle/working/training_history.pkl\", \"wb\") as f:\n#     pickle.dump(history, f)\n\nprint(\"  Note: To plot real loss curves, save 'history' dict in Part 4:\")\nprint(\"   with open('/kaggle/working/training_history.pkl', 'wb') as f:\")\nprint(\"       pickle.dump(history, f)\")\nprint(\"\\n   For now, showing example plot structure...\\n\")\n\n# Example plot structure (replace with real data)\nepochs = range(1, 11)\nexample_train_loss = [3.2, 2.8, 2.5, 2.3, 2.1, 2.0, 1.9, 1.85, 1.8, 1.75]\nexample_val_loss = [3.1, 2.9, 2.6, 2.4, 2.2, 2.1, 2.0, 1.95, 1.9, 1.85]\n\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, example_train_loss, 'b-o', label='Train Loss', linewidth=2)\nplt.plot(epochs, example_val_loss, 'r-s', label='Val Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training and Validation Loss Over Epochs', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('/kaggle/working/loss_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"  Loss curves saved â†’ /kaggle/working/loss_curves.png\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  3. QUANTITATIVE EVALUATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*60)\nprint(\"         3. QUANTITATIVE EVALUATION\")\nprint(\"=\"*60 + \"\\n\")\n\n# Generate captions for all test images\nprint(\"  Generating captions for all test images...\")\n\nreferences = []  # List of lists of reference tokens\nhypotheses = []  # List of hypothesis tokens\nall_ref_tokens = []  # Flattened for token-level metrics\nall_hyp_tokens = []  # Flattened for token-level metrics\n\nfor idx in range(min(1000, len(test_df))):  # Limit to 1000 for speed\n    sample_row = test_df.iloc[idx]\n    img_name = sample_row['image']\n    \n    # Get all reference captions for this image\n    img_refs = test_df[test_df['image'] == img_name]['tokens'].tolist()\n    # Remove <start> and <end> tokens\n    img_refs = [[t for t in ref if t not in ['<start>', '<end>']] for ref in img_refs]\n    references.append(img_refs)\n    \n    # Generate hypothesis\n    feature = torch.tensor(features_dict[img_name], dtype=torch.float32)\n    generated = beam_search(model, feature, vocab, beam_width=5, \n                           max_len=MAX_LEN, device=device)\n    hypotheses.append(generated)\n    \n    # For token-level metrics, use first reference\n    all_ref_tokens.extend(img_refs[0])\n    all_hyp_tokens.extend(generated)\n\nprint(f\"  Generated {len(hypotheses)} captions\\n\")\n\n# â”€â”€ BLEU-4 Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"  Calculating BLEU-4 Score...\")\nsmoothing = SmoothingFunction().method1\nbleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25),\n                    smoothing_function=smoothing)\nprint(f\"    BLEU-4 : {bleu4:.4f}\\n\")\n\n# â”€â”€ Token-level Precision, Recall, F1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"  Calculating Token-level Metrics...\")\n\n# Create vocabulary mapping for sklearn\nunique_tokens = list(set(all_ref_tokens + all_hyp_tokens))\ntoken_to_idx = {token: idx for idx, token in enumerate(unique_tokens)}\n\n# Convert to indices\nref_indices = [token_to_idx.get(t, 0) for t in all_ref_tokens]\nhyp_indices = [token_to_idx.get(t, 0) for t in all_hyp_tokens]\n\n# Pad to same length\nmax_len = max(len(ref_indices), len(hyp_indices))\nref_indices += [0] * (max_len - len(ref_indices))\nhyp_indices += [0] * (max_len - len(hyp_indices))\n\nprecision = precision_score(ref_indices, hyp_indices, average='weighted', zero_division=0)\nrecall = recall_score(ref_indices, hyp_indices, average='weighted', zero_division=0)\nf1 = f1_score(ref_indices, hyp_indices, average='weighted', zero_division=0)\n\nprint(f\"    Precision : {precision:.4f}\")\nprint(f\"    Recall    : {recall:.4f}\")\nprint(f\"    F1-Score  : {f1:.4f}\\n\")\n\n# â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"=\"*60)\nprint(\"         EVALUATION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"  BLEU-4    : {bleu4:.4f}\")\nprint(f\"  Precision : {precision:.4f}\")\nprint(f\"  Recall    : {recall:.4f}\")\nprint(f\"  F1-Score  : {f1:.4f}\")\nprint(\"=\"*60 + \"\\n\")\n\n# Save metrics\nmetrics = {\n    \"bleu4\": bleu4,\n    \"precision\": precision,\n    \"recall\": recall,\n    \"f1\": f1\n}\nwith open(\"/kaggle/working/evaluation_metrics.pkl\", \"wb\") as f:\n    pickle.dump(metrics, f)\n\nprint(f\"  Metrics saved â†’ /kaggle/working/evaluation_metrics.pkl\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  4. GRADIO APP DEPLOYMENT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"=\"*60)\nprint(\"         4. GRADIO APP DEPLOYMENT\")\nprint(\"=\"*60 + \"\\n\")\n\ndef generate_caption_from_upload(image):\n    \"\"\"\n    Gradio function: takes uploaded image, generates caption.\n    \n    Args:\n        image : PIL Image or numpy array\n    Returns:\n        caption : string\n    \"\"\"\n    if image is None:\n        return \"Please upload an image!\"\n    \n    # Convert to PIL if needed\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image).convert('RGB')\n    \n    # Preprocess image (same as Part 1)\n    from torchvision import transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                           std=(0.229, 0.224, 0.225)),\n    ])\n    \n    # Extract features using ResNet50\n    from torchvision import models\n    backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n    backbone = nn.Sequential(*list(backbone.children())[:-1])\n    backbone = backbone.to(device)\n    backbone.eval()\n    \n    img_tensor = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feature = backbone(img_tensor).view(-1).cpu()\n    \n    # Generate caption\n    greedy_cap = greedy_search(model, feature, vocab, max_len=MAX_LEN, device=device)\n    beam_cap = beam_search(model, feature, vocab, beam_width=5, max_len=MAX_LEN, device=device)\n    \n    result = f\"**Greedy Search:**\\n{' '.join(greedy_cap)}\\n\\n\"\n    result += f\"**Beam Search (width=5):**\\n{' '.join(beam_cap)}\"\n    \n    return result\n\n\n# Create Gradio Interface\ndemo = gr.Interface(\n    fn=generate_caption_from_upload,\n    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n    outputs=gr.Textbox(label=\"Generated Caption\", lines=4),\n    title=\"ğŸ–¼ï¸ Image Caption Generator\",\n    description=\"Upload an image and get an AI-generated caption using Seq2Seq model with ResNet50 features.\",\n    examples=[\n        [os.path.join(IMAGE_DIR, test_df.iloc[0]['image'])],\n        [os.path.join(IMAGE_DIR, test_df.iloc[1]['image'])],\n    ] if os.path.exists(IMAGE_DIR) else None,\n    theme=\"default\"\n)\n\nprint(\"  Launching Gradio app...\")\nprint(\"    Access the demo using the public URL below\\n\")\n\n# Launch app\ndemo.launch(share=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"         PART 5 COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\n  Caption examples    â†’ /kaggle/working/caption_examples.png\")\nprint(f\"  Loss curves         â†’ /kaggle/working/loss_curves.png\")\nprint(f\"  Evaluation metrics  â†’ /kaggle/working/evaluation_metrics.pkl\")\nprint(f\"  Gradio demo         â†’ Running (click link above)\")\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:17:39.900317Z","iopub.execute_input":"2026-02-11T16:17:39.901003Z","iopub.status.idle":"2026-02-11T16:18:58.551961Z","shell.execute_reply.started":"2026-02-11T16:17:39.900971Z","shell.execute_reply":"2026-02-11T16:18:58.551236Z"}},"outputs":[],"execution_count":null}]}